{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ec4627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6ff5df",
   "metadata": {},
   "source": [
    "![title](https://raw.githubusercontent.com/google/eng-edu/main/ml/recommendation-systems/images/softmax-model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b20294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7440df29",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d8fb119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fashion_recommendations.metrics.average_precision import mapk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e231dba",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ef83f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle, islice\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import IterableDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c04a0081",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionDatasetSingleLabel(IterableDataset):\n",
    "\n",
    "    def __init__(self, dataset_filepath, max_length, padding_value):\n",
    "        \n",
    "        self.dataset_itr = open(dataset_filepath, 'r')\n",
    "        next(self.dataset_itr)  # skip header\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.padding_value = padding_value\n",
    "    \n",
    "    def process_label(self, label: str):\n",
    "\n",
    "        return torch.tensor(int(label))\n",
    "    \n",
    "    def process_input(self, input_str: str, max_length, padding_value):\n",
    "        \n",
    "        input_tensor = torch.tensor([int(v) for v in input_str.split(',')])\n",
    "        \n",
    "        len_orig = len(input_tensor)\n",
    "        \n",
    "        if len_orig >= max_length:\n",
    "            \n",
    "            input_tensor = input_tensor[-max_length:]  # Take latest items\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            num_pad = max_length - len_orig\n",
    "            \n",
    "            input_tensor = F.pad(input_tensor, (0, num_pad), value=padding_value)\n",
    "            \n",
    "        return input_tensor\n",
    "    \n",
    "    def parse_itr(self, dataset_itr):\n",
    "        \n",
    "        for line in dataset_itr:\n",
    "        \n",
    "            line_items = line.rstrip('\\n').split('\\t')  # [customer_id, label, input]\n",
    "            \n",
    "            label = self.process_label(line_items[1])\n",
    "            \n",
    "            input_seq = self.process_input(line_items[2], self.max_length, self.padding_value)\n",
    "\n",
    "            yield input_seq, label\n",
    "        \n",
    "    def get_stream(self, dataset_itr):\n",
    "        \n",
    "        return self.parse_itr(dataset_itr)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \n",
    "        return self.get_stream(self.dataset_itr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9a0eb5",
   "metadata": {},
   "source": [
    "### fashrec-v1\n",
    "\n",
    "- Use order history only\n",
    "- Embedding for each item\n",
    "- If item ordered X times index its embedding X times\n",
    "- BoW of all item history embeddings\n",
    "- For each customer take random transaction and roll-back data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48100cb",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec0d2d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105542"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_num_articles = pd.read_csv('data/articles.csv').shape[0]\n",
    "total_num_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8d29b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionRecV1(nn.Module):\n",
    "\n",
    "    def __init__(self, mask_value, embedding_dim):\n",
    "        super(FashionRecV1, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=total_num_articles, embedding_dim=self.embedding_dim)\n",
    "        \n",
    "        self.fc_1 = nn.Linear(in_features=self.embedding_dim, out_features=11)\n",
    "        \n",
    "        self.fc_2 = nn.Linear(in_features=11, out_features=total_num_articles)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        self.mask_value = mask_value\n",
    "\n",
    "    def forward(self, padded_sequences):\n",
    "        \n",
    "        x = self.embedding(padded_sequences)\n",
    "\n",
    "        mask = padded_sequences != self.mask_value\n",
    "        \n",
    "        # Repeat mask so that shape matches output of embedding\n",
    "        mask = torch.unsqueeze(mask, dim=2)\n",
    "        mask = mask.repeat(1, 1, self.embedding_dim)\n",
    "\n",
    "        x = mask * x\n",
    "        \n",
    "        # Compute average over non-padding embeddings:\n",
    "        x = x.sum(dim=1)\n",
    "        num_embeddings = mask[:, :, 0].sum(dim=1).reshape(-1, 1)  # Divide by number of (non-padding) embeddings to get mean embedding\n",
    "\n",
    "        x = torch.div(\n",
    "            x, \n",
    "            num_embeddings\n",
    "        )\n",
    "\n",
    "        x = self.fc_1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc_2(x)\n",
    "        x = self.tanh(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ea03a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6d1b82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_training_examples = pd.read_csv('data/splits/train_single_purchase_label_sample.tsv', sep='\\t', low_memory=False).shape[0]\n",
    "total_batches = np.ceil(total_training_examples/BATCH_SIZE)\n",
    "total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "77eed2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72019"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_dev_examples = pd.read_csv('data/splits/dev_single_purchase_label.tsv', sep='\\t', low_memory=False).shape[0]\n",
    "total_dev_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c99cc4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_actuals = pd.read_csv('data/splits/dev_all_purchase_label.tsv', sep='\\t')['article_id_idx_historical'].str.split(',').apply(lambda x: [int(i) for i in x]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "773c00fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING_VALUE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "25e3578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_rec_v1 = FashionRecV1(mask_value=PADDING_VALUE, embedding_dim=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c95e9dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=fashion_rec_v1.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da972de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_INFERENCE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6616f106",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb8a00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_losses = []\n",
    "dev_losses = []\n",
    "dev_maps = []\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    \n",
    "    # Since we use an IterableDataset we need to reinstaniate the dataset since file end will have been reached:\n",
    "    train_dataset = FashionDatasetSingleLabel(dataset_filepath='data/splits/train_single_purchase_label_sample.tsv', max_length=50, padding_value=0)    \n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)  \n",
    "    \n",
    "    for idx, data in enumerate(tqdm(train_loader, total=total_batches)):\n",
    "        \n",
    "        X, y = data\n",
    "        \n",
    "        X = X.long()\n",
    "\n",
    "        optimizer.zero_grad()  # Set gradients to 0 otherwise will accumulate\n",
    "\n",
    "        y_pred = fashion_rec_v1(X)\n",
    "        \n",
    "        loss = criterion(y_pred, y)  # Need index for loss in PyTorch\n",
    "\n",
    "        loss.backward()    \n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    if idx % 100 == 0:\n",
    "\n",
    "        # Compute train loss\n",
    "        train_dataset_for_loss = FashionDatasetSingleLabel(dataset_filepath='data/splits/train_single_purchase_label_sample.tsv', max_length=50, padding_value=0)     \n",
    "        total_train_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in DataLoader(train_dataset_for_loss, batch_size=BATCH_SIZE_INFERENCE):\n",
    "                X, y = data\n",
    "                optimizer.zero_grad()  # Set gradients to 0 otherwise will accumulate\n",
    "                y_pred = fashion_rec_v1(X)\n",
    "                loss = nn.CrossEntropyLoss(reduction='sum')(y_pred, y).item()\n",
    "                total_train_loss += loss\n",
    "\n",
    "            mean_train_loss = total_train_loss / total_training_examples\n",
    "\n",
    "            print(f\"Training loss: {mean_train_loss}\")\n",
    "            training_losses.append(mean_train_loss)\n",
    "\n",
    "        # Compute dev loss\n",
    "        dev_dataset_for_loss = FashionDatasetSingleLabel(dataset_filepath='data/splits/dev_single_purchase_label.tsv', max_length=50, padding_value=0)     \n",
    "        total_dev_loss = 0\n",
    "        top_12_predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in DataLoader(dev_dataset_for_loss, batch_size=BATCH_SIZE_INFERENCE):\n",
    "                X, y = data\n",
    "                optimizer.zero_grad()  # Set gradients to 0 otherwise will accumulate\n",
    "                y_pred = fashion_rec_v1(X)\n",
    "                loss = nn.CrossEntropyLoss(reduction='sum')(y_pred, y).item()\n",
    "                total_dev_loss += loss\n",
    "\n",
    "                top_12 = y_pred.argsort(dim=1, descending=True)[:, :12].tolist()\n",
    "                top_12_predictions += top_12\n",
    "\n",
    "            mean_dev_loss = total_dev_loss / total_dev_examples\n",
    "\n",
    "            print(f\"Dev loss: {mean_dev_loss}\")\n",
    "            dev_losses.append(mean_dev_loss)\n",
    "\n",
    "        # Compute dev MAP@12\n",
    "        dev_mapk12 = mapk(dev_actuals, top_12_predictions, k=12)\n",
    "        print(f\"Dev MAP@12: {dev_mapk12}\")\n",
    "        dev_maps.append(dev_mapk12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce1a36d",
   "metadata": {},
   "source": [
    "TODO\n",
    "- Add shuffle to datasets (or pre-shuffle)\n",
    "- Add to_device and check that it's GPU\n",
    "- Add code for generating predictions for submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
